{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NLP-lab :  Plongements de mots (word embeddings)\n",
    "\n",
    "                                            Christopher Kermorvant\n",
    "\n",
    "                            “The meaning of a word can be inferred by the company it keeps”\n",
    "\n",
    "Dans cette série d'exercices, nous allons explorer  trois  plongements (embeddings) de mots :\n",
    "\n",
    "*  [Collobert & Weston](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf) https://ronan.collobert.com/senna/\n",
    "* [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "* [BERT](https://huggingface.co/bert-base-uncased) \n",
    "\n",
    "   \n",
    "Pour les deux premiers, nous examinerons les mots les plus proches et visualiserons leurs positions dans l'espaces après réduction de dimension. Puis nous procéderons à des [évaluations](https://arxiv.org/pdf/1801.09536.pdf) qualitatives et intrinsèques des embeddings.\n",
    "\n",
    "Enfin nous étudierons les raisonnements par analogies que l'on peut conduire par l'arithmétique sur les embeddings (et leurs biais).\n",
    "\n",
    "Pour BERT, nous étudierons la représentation d'un mot polysémique en fonction de son contexte.\n",
    "\n",
    "Dans le code déjà fourni, ajouter votre code à l'endroit indiqué par `YOUR CODE HERE`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "\n",
    "# disable warnings for libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# configure logger\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Les fichiers d'embeddings pré-entraînés\n",
    "\n",
    "Téléchargez dans `data` les fichiers contenant les embeddings :\n",
    "* Collobert (taille 50) : [collobert_embeddings.txt.zip](https://storage.teklia.com/shared/deepnlp-labs/collobert_embeddings.txt.zip) qui contient les vecteurs d'embeddings  et [collobert_words.lst](https://storage.teklia.com/shared/deepnlp-labs/collobert_words.lst) qui contient les mots associés;\n",
    "* Glove (taille 50):  [glove.6B.50d.txt.zip](https://storage.teklia.com/shared/deepnlp-labs/glove.6B.50d.txt.zip) qui contient à la fois les vecteurs et les mots.\n",
    "\n",
    "Il faut décompresser les fichiers pour pouvoir les charger.\n",
    "\n",
    "N'hésitez pas à ouvrir les fichiers pour voir ce qu'ils contiennent (c'est parfois surprennant).\n",
    "\n",
    "#### Question : \n",
    ">* Donner la taille des fichiers d'embeddings avant unzip\n",
    ">* En explorant le contenu des fichiers d'embedding, donner le nombre de mots pour lesquels ces fichiers fournissent des embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploration des embeddings\n",
    "\n",
    "### Liste des mots les plus proches\n",
    "\n",
    "L'objectif de cet exercice est de lister les mots les plus proches d'un mot donné pour l'embeddings Collobert. Dans un premier temps, nous allons charger les vecteurs de l'embedding Collobert dans un array numpy et les mots associés dans une liste python. Ensuite, nous utiliserons la structure de données [KDTree de scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) pour faire une recherche rapide des vecteurs les plus proches d'une série de mots.\n",
    "\n",
    "### Chargement des embeddings\n",
    "\n",
    "#### Question : \n",
    ">* charger les vecteurs d'embeddings à partir du fichier `data/collobert_embeddings.txt` en utilisant la fonction numpy [genfromtxt](https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html)\n",
    ">* charger dans une liste python les mots associés aux vecteurs à partir du fichier `data/collobert_words.lst` (avec `open()` et `readlines()`)\n",
    ">* vérifiez que les tailles sont correctes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# YOUR CODE HERE\n",
    "vect = np.genfromtxt('data/collobert_embeddings.txt')\n",
    "with open('data/collobert_words.lst', 'r') as fichier:\n",
    "    mots=fichier.readlines()\n",
    "for i in range(len(mots)):\n",
    "    mots[i]=mots[i].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les arbres KD (KD tree) sont une structure de données très efficace pour stocker de grands ensemble de points dans une espace multi-dimensionnel et faire des recherches très efficaces de plus proches voisins. \n",
    "\n",
    "#### Question \n",
    "> * Initialisez la structure de [KDTree](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) avec les vecteurs d'embeddings de Collobert\n",
    "> * En utilisant la fonction [tree.query](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.query.html#scipy.spatial.KDTree.query), afficher les 5 mots les plus proches des mots suivants : 'mother', 'computer', 'dentist', 'war', 'president', 'secretary', 'nurse' \n",
    "     * *Indice : vous pouvez utiliser la fonction `collobert_words.index(w)` pour obtenir l'indice d'un mot dans la liste des mots*\n",
    "> * Créer une liste `words_plus_neighbors` contenant les mots et tous leurs voisins (pour la question suivante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "# YOUR CODE HERE\n",
    "tree=spatial.KDTree(vect) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mother',\n",
       " 'mother',\n",
       " 'daughter',\n",
       " 'wife',\n",
       " 'father',\n",
       " 'husband',\n",
       " 'computer',\n",
       " 'computer',\n",
       " 'laptop',\n",
       " 'multimedia',\n",
       " 'desktop',\n",
       " 'software',\n",
       " 'dentist',\n",
       " 'dentist',\n",
       " 'pharmacist',\n",
       " 'midwife',\n",
       " 'physician',\n",
       " 'housekeeper',\n",
       " 'war',\n",
       " 'war',\n",
       " 'revolution',\n",
       " 'death',\n",
       " 'court',\n",
       " 'independence',\n",
       " 'president',\n",
       " 'president',\n",
       " 'governor',\n",
       " 'chairman',\n",
       " 'mayor',\n",
       " 'secretary',\n",
       " 'secretary',\n",
       " 'secretary',\n",
       " 'minister',\n",
       " 'treasurer',\n",
       " 'chairman',\n",
       " 'commissioner',\n",
       " 'nurse',\n",
       " 'nurse',\n",
       " 'physician',\n",
       " 'veterinarian',\n",
       " 'dentist',\n",
       " 'surgeon']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m=['mother', 'computer', 'dentist', 'war', 'president', 'secretary', 'nurse']\n",
    "words_plus_neighbors=[]\n",
    "for i in m:\n",
    "    words_plus_neighbors.append(i)\n",
    "    ind=mots.index(i)\n",
    "    l=tree.query(vect[ind],k=5)\n",
    "    words_plus_neighbors+=[mots[j] for j in l[1]]\n",
    "\n",
    "words_plus_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation avec T-SNE\n",
    "\n",
    "Les embeddings sont des vecteurs de plusieurs centaines de dimensions. Il n'est donc pas possible de les visualiser dans leur espace d'origine. Il est par contre possible d'appliquer des algorithmes de réduction de dimension pour les visualiser en 2 ou 3 dimension. Un des algorithmes de réduction de dimension permettant une visualisation en 2D est [tSNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). \n",
    "\n",
    "#### Question\n",
    "> * créer un object `word_vectors` de type `np.array` à partir d'une liste contenant tous les embeddings des mots de la liste `words_plus_neighbors`\n",
    "> * créer un objet tSNE à partir de la librairie `from sklearn.manifold import TSNE` avec les paramètres `random_state=0`, `n_iter=2000` et `perplexity=15.0` pour une visualisation en 2 dimensions\n",
    "> * Calculer *T* la transformation tSNE des vecteur `word_vectors` en appliquant la function `.fit_transform(word_vectors)` à l'objet tSNE. Cette fonction estime les paramètres de la transformation tSNE et retourne la représentation en dimension réduite des vecteurs utilisés pour l'estimation.\n",
    "> * Utiliser la fonction `scatterplot` de [seaborn](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) pour représenter les points en 2 dimensions  et ajouter les labels des mots avec la function `plt.annotate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guspo\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\guspo\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14056\\1943726505.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# retrieve the word representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mword_vectors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmots\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_plus_neighbors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# create the tSNE transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# graphics\n",
    "import matplotlib.pyplot as plt\n",
    "# display matplotlib graphics in notebook\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "# retrieve the word representation\n",
    "# YOUR CODE HERE\n",
    "word_vectors=np.array([np.array(vect[mots.index(i)]) for i in words_plus_neighbors])\n",
    "\n",
    "# create the tSNE transform\n",
    "print(word_vectors)\n",
    "T = TSNE(n_components=2, random_state=0, n_iter=2000, perplexity=15.0).fit_transform(word_vectors)\n",
    "print(T)\n",
    "# fit and transform the word vectors, store in T\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('#f9f9f9')\n",
    "\n",
    "sns.set(rc={'figure.figsize':(14, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "sns.scatterplot(x=T[:, 0], y=T[:, 1])\n",
    "\n",
    "for label, x, y in zip(words_plus_neighbors, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation des embeddings \n",
    "\n",
    "### Évaluation intrinsèque\n",
    "\n",
    "[A Survey of Word Embeddings Evaluation Methods](https://arxiv.org/pdf/1801.09536.pdf), Bakarov, 2018.\n",
    "\n",
    "\n",
    ">les distances entre les mots dans un espace vectoriel pourraient être évaluées à l'aide des jugements heuristiques humains sur les distances sémantiques réelles entre ces mots (par exemple, la distance entre tasse et gobelet définies dans un intervalle continu 0, 1 serait 0.8 puisque ces mots sont synonymes, mais pas vraiment la même chose).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Téléchargement des datasets pré-établis et annotés manuellement\n",
    "\n",
    "Nous allons utiliser 4 jeux de données  pour évaluer la qualité des embeddings : [MEN](http://clic.cimec.unitn.it/~elia.bruni/MEN.html), [WS353R](http://www.aclweb.org/anthology/N09-1003.pdf), [SimLex999](http://leviants.com/ira.leviant/MultilingualVSMdata.html) et [MTurk](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.205.8607&rep=rep1&type=pdf). \n",
    "\n",
    "\n",
    "Ces jeux de données contiennent des paires de mots dont la proximité sémantique a été évaluée manuellement par des humains. Pour chaque dataset, dataset.X contient une liste de paires de mots et dataset.y contient le score de proximité pour chaque paire.\n",
    "\n",
    "* MEN, 3 000 paires évaluées par relation sémantique avec une échelle discrète de 0 à 50\n",
    "* SimLex-999, 999 paires évaluées avec un fort respect pour la similarité sémantique avec une échelle de 0 à 10\n",
    "* MTurk-287, 287 paires évaluées par relation sémantique avec une échelle de 0 à 5\n",
    "* WordSim-353, 353 paires évaluées par similarité sémantique (cependant, certains chercheurs trouvent les instructions pour les évaluateurs ambiguës en ce qui concerne la similarité et l'association) sur une échelle de 0 à 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polyglot\n",
      "  Downloading polyglot-16.7.4.tar.gz (126 kB)\n",
      "     -------------------------------------- 126.3/126.3 kB 2.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: polyglot\n",
      "  Building wheel for polyglot (setup.py): started\n",
      "  Building wheel for polyglot (setup.py): finished with status 'done'\n",
      "  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52559 sha256=f686a833dfd912f902d07feb4891356c87fa213d269d712d1f4484404fbfcf2e\n",
      "  Stored in directory: c:\\users\\guspo\\appdata\\local\\pip\\cache\\wheels\\77\\4a\\9d\\5141018da475375d91dc1af07520b1f2b077579f2f55353afb\n",
      "Successfully built polyglot\n",
      "Installing collected packages: polyglot\n",
      "Successfully installed polyglot-16.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    " Functions for fetching similarity datasets\n",
    "\n",
    "https://github.com/kudkudak/word-embeddings-benchmarks\n",
    "\n",
    "Jastrzebski, Stanisław, Damian Leśniak, and Wojciech Marian Czarnecki. \n",
    "\"How to evaluate word embeddings? on importance of data efficiency \n",
    "and simple supervised tasks.\" arXiv preprint arXiv:1702.02170 (2017).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os, scipy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import Bunch\n",
    "from polyglot.mapping import Embedding\n",
    "import requests\n",
    "from utils import _fetch_file\n",
    "def _get_as_pd(url, dataset_name, **read_csv_kwargs):\n",
    "    return pd.read_csv(_fetch_file(url, dataset_name, verbose=0), **read_csv_kwargs)\n",
    "\n",
    "\n",
    "def _partial_fetch(_file):\n",
    "    print('Downloading', _file)\n",
    "\n",
    "def fetch_MTurk():\n",
    "    \"\"\"\n",
    "    Fetch MTurk dataset for testing attributional similarity\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : sklearn.datasets.base.Bunch\n",
    "        dictionary-like object. Keys of interest:\n",
    "        'X': matrix of 2 words per column,\n",
    "        'y': vector with scores,\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Radinsky, Kira et al., \"A Word at a Time: Computing Word Relatedness Using Temporal Semantic Analysis\", 2011\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Human labeled examples of word semantic relatedness. The data pairs were generated using an algorithm as\n",
    "    described in the paper by [K. Radinsky, E. Agichtein, E. Gabrilovich, S. Markovitch.].\n",
    "    Each pair of words was evaluated by 10 people on a scale of 1-5.\n",
    "\n",
    "    Additionally scores were multiplied by factor of 2.\n",
    "    \"\"\"\n",
    "    _partial_fetch('MTurk dataset: attributional similarity')\n",
    "    data = _get_as_pd('https://www.dropbox.com/s/f1v4ve495mmd9pw/EN-TRUK.txt?dl=1',\n",
    "                      'similarity', header=None, sep=\" \").values\n",
    "    return Bunch(X=data[:, 0:2].astype(\"object\"),\n",
    "                 y=2 * data[:, 2].astype(float))\n",
    "\n",
    "\n",
    "def fetch_MEN(which=\"all\", form=\"natural\"):\n",
    "    \"\"\"\n",
    "    Fetch MEN dataset for testing similarity and relatedness\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    which : \"all\", \"test\" or \"dev\"\n",
    "    form : \"lem\" or \"natural\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : sklearn.datasets.base.Bunch\n",
    "        dictionary-like object. Keys of interest:\n",
    "        'X': matrix of 2 words per column,\n",
    "        'y': vector with scores\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Published at http://clic.cimec.unitn.it/~elia.bruni/MEN.html.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Scores for MEN are calculated differently than in WS353 or SimLex999.\n",
    "    Furthermore scores where rescaled to 0 - 10 scale to match standard scaling.\n",
    "\n",
    "    The MEN Test Collection contains two sets of English word pairs (one for training and one for testing)\n",
    "    together with human-assigned similarity judgments, obtained by crowdsourcing using Amazon Mechanical\n",
    "    Turk via the CrowdFlower interface. The collection can be used to train and/or test computer algorithms\n",
    "    implementing semantic similarity and relatedness measures.\n",
    "    \"\"\"\n",
    "    _partial_fetch('MEN dataset: similarity and relatedness')\n",
    "    if which == \"dev\":\n",
    "        data = _get_as_pd('https://www.dropbox.com/s/c0hm5dd95xapenf/EN-MEN-LEM-DEV.txt?dl=1',\n",
    "                          'similarity', header=None, sep=\" \")\n",
    "    elif which == \"test\":\n",
    "        data = _get_as_pd('https://www.dropbox.com/s/vdmqgvn65smm2ah/EN-MEN-LEM-TEST.txt?dl=1',\n",
    "                          'similarity/EN-MEN-LEM-TEST', header=None, sep=\" \")\n",
    "    elif which == \"all\":\n",
    "        data = _get_as_pd('https://www.dropbox.com/s/b9rv8s7l32ni274/EN-MEN-LEM.txt?dl=1',\n",
    "                          'similarity', header=None, sep=\" \")\n",
    "    else:\n",
    "        raise RuntimeError(\"Not recognized which parameter\")\n",
    "\n",
    "    if form == \"natural\":\n",
    "        # Remove last two chars from first two columns\n",
    "        data = data.apply(lambda x: [y if isinstance(y, float) else y[0:-2] for y in x])\n",
    "    elif form != \"lem\":\n",
    "        raise RuntimeError(\"Not recognized form argument\")\n",
    "\n",
    "    return Bunch(X=data.values[:, 0:2].astype(\"object\"), y=data.values[:, 2:].astype(float) / 5.0)\n",
    "\n",
    "\n",
    "def fetch_WS353(which=\"all\"):\n",
    "    \"\"\"\n",
    "    Fetch WS353 dataset for testing attributional and\n",
    "    relatedness similarity\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    which : 'all': for both relatedness and attributional similarity,\n",
    "            'relatedness': for relatedness similarity\n",
    "            'similarity': for attributional similarity\n",
    "            'set1': as divided by authors\n",
    "            'set2': as divided by authors\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Finkelstein, Gabrilovich, \"Placing Search in Context: The Concept Revisited†\", 2002\n",
    "    Agirre, Eneko et al., \"A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches\",\n",
    "    2009\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : sklearn.datasets.base.Bunch\n",
    "        dictionary-like object. Keys of interest:\n",
    "        'X': matrix of 2 words per column,\n",
    "        'y': vector with scores,\n",
    "        'sd': vector of std of scores if available (for set1 and set2)\n",
    "    \"\"\"\n",
    "    _partial_fetch('WS353 dataset: attributional and relatedness similarity')\n",
    "    if which == \"all\":\n",
    "        data = _get_as_pd('https://www.dropbox.com/s/eqal5qj97ajaycz/EN-WS353.txt?dl=1',\n",
    "                          'similarity', header=0, sep=\"\\t\")\n",
    "    elif which == \"relatedness\":\n",
    "        data = _get_as_pd('https://www.dropbox.com/s/x94ob9zg0kj67xg/EN-WSR353.txt?dl=1',\n",
    "                          'similarity', header=None, sep=\"\\t\")\n",
    "    elif which == \"similarity\":\n",
    "        data = _get_as_pd('https://www.dropbox.com/s/ohbamierd2kt1kp/EN-WSS353.txt?dl=1',\n",
    "                          'similarity', header=None, sep=\"\\t\")\n",
    "    elif which == \"set1\":\n",
    "        data = _get_as_pd('https://www.dropbox.com/s/opj6uxzh5ov8gha/EN-WS353-SET1.txt?dl=1',\n",
    "                          'similarity', header=0, sep=\"\\t\")\n",
    "    elif which == \"set2\":\n",
    "        data = _get_as_pd('https://www.dropbox.com/s/w03734er70wyt5o/EN-WS353-SET2.txt?dl=1',\n",
    "                          'similarity', header=0, sep=\"\\t\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Not recognized which parameter\")\n",
    "\n",
    "    # We basically select all the columns available\n",
    "    X = data.values[:, 0:2]\n",
    "    y = data.values[:, 2].astype(float)\n",
    "\n",
    "    # We have also scores\n",
    "    if data.values.shape[1] > 3:\n",
    "        sd = np.std(data.values[:, 2:15].astype(float), axis=1).flatten()\n",
    "        return Bunch(X=X.astype(\"object\"), y=y, sd=sd)\n",
    "    else:\n",
    "        return Bunch(X=X.astype(\"object\"), y=y)\n",
    "\n",
    "\n",
    "def fetch_RG65():\n",
    "    \"\"\"\n",
    "    Fetch Rubenstein and Goodenough dataset for testing attributional and\n",
    "    relatedness similarity\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : sklearn.datasets.base.Bunch\n",
    "        dictionary-like object. Keys of interest:\n",
    "        'X': matrix of 2 words per column,\n",
    "        'y': vector with scores,\n",
    "        'sd': vector of std of scores if available (for set1 and set2)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Rubenstein, Goodenough, \"Contextual correlates of synonymy\", 1965\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Scores were scaled by factor 10/4\n",
    "    \"\"\"\n",
    "    _partial_fetch('Rubenstein and Goodenough dataset: attributional and relatedness similarity')\n",
    "    data = _get_as_pd('https://www.dropbox.com/s/chopke5zqly228d/EN-RG-65.txt?dl=1',\n",
    "                      'similarity', header=None, sep=\"\\t\").values\n",
    "\n",
    "    return Bunch(X=data[:, 0:2].astype(\"object\"),\n",
    "                 y=data[:, 2].astype(float) * 10.0 / 4.0)\n",
    "\n",
    "\n",
    "def fetch_RW():\n",
    "    \"\"\"\n",
    "    Fetch Rare Words dataset for testing attributional similarity\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : sklearn.datasets.base.Bunch\n",
    "        dictionary-like object. Keys of interest:\n",
    "        'X': matrix of 2 words per column,\n",
    "        'y': vector with scores,\n",
    "        'sd': vector of std of scores\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Published at http://www-nlp.stanford.edu/~lmthang/morphoNLM/.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    2034 word pairs that are relatively rare with human similarity scores. Rare word selection: our choices of\n",
    "    rare words (word1) are based on their frequencies – based on five bins (5, 10], (10, 100], (100, 1000],\n",
    "    (1000, 10000], and the affixes they possess. To create a diverse set of candidates, we randomly\n",
    "    select 15 words for each configuration (a frequency bin, an affix). At the scale of Wikipedia,\n",
    "    a word with frequency of 1-5 is most likely a junk word, and even restricted to words with\n",
    "    frequencies above five, there are still many non-English words. To counter such problems,\n",
    "    each word selected is required to have a non-zero number of synsets in WordNet(Miller, 1995).\n",
    "    \"\"\"\n",
    "    _partial_fetch('Rare Words dataset: attributional similarity')\n",
    "    data = _get_as_pd('https://www.dropbox.com/s/xhimnr51kcla62k/EN-RW.txt?dl=1',\n",
    "                      'similarity', header=None, sep=\"\\t\").values\n",
    "    return Bunch(X=data[:, 0:2].astype(\"object\"),\n",
    "                 y=data[:, 2].astype(float),\n",
    "                 sd=np.std(data[:, 3:].astype(float)))\n",
    "\n",
    "\n",
    "def fetch_multilingual_SimLex999(which=\"EN\"):\n",
    "    \"\"\"\n",
    "    Fetch Multilingual SimLex999 dataset for testing attributional similarity\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    which : \"EN\", \"RU\", \"IT\" or \"DE\" for language\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : sklearn.datasets.base.Bunch\n",
    "        dictionary-like object. Keys of interest:\n",
    "        'X': matrix of 2 words per column,\n",
    "        'y': vector with scores,\n",
    "        'sd': vector of sd of scores,\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Published at http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Scores for EN are different than the original SimLex999 dataset.\n",
    "\n",
    "    Authors description:\n",
    "    Multilingual SimLex999 resource consists of translations of the SimLex999 word similarity data set to\n",
    "    three languages: German, Italian and Russian. Each of the translated datasets is scored by\n",
    "    13 human judges (crowdworkers) - all fluent speakers of its language. For consistency, we\n",
    "    also collected human judgments for the original English corpus according to the same protocol\n",
    "    applied to the other languages. This dataset allows to explore the impact of the \"judgement language\"\n",
    "    (the language in which word pairs are presented to the human judges) on the resulted similarity scores\n",
    "    and to evaluate vector space models on a truly multilingual setup (i.e. when both the training and the\n",
    "    test data are multilingual).\n",
    "    \"\"\"\n",
    "    _partial_fetch('Multilingual SimLex999 dataset: attributional similarity')\n",
    "    if which == \"EN\":\n",
    "        data = _get_as_pd('https://www.dropbox.com/s/nczc4ao6koqq7qm/EN-MSIM999.txt?dl=1',\n",
    "                          'similarity', header=None, encoding='utf-8', sep=\" \")\n",
    "    elif which == \"DE\":\n",
    "        data = _get_as_pd('https://www.dropbox.com/s/ucpwrp0ahawsdtf/DE-MSIM999.txt?dl=1',\n",
    "                          'similarity', header=None, encoding='utf-8', sep=\" \")\n",
    "    elif which == \"IT\":\n",
    "        data = _get_as_pd('https://www.dropbox.com/s/siqjagyz8dkjb9q/IT-MSIM999.txt?dl=1',\n",
    "                          'similarity', header=None, encoding='utf-8', sep=\" \")\n",
    "    elif which == \"RU\":\n",
    "        data = _get_as_pd('https://www.dropbox.com/s/3v26edm9a31klko/RU-MSIM999.txt?dl=1',\n",
    "                          'similarity', header=None, encoding='utf-8', sep=\" \")\n",
    "    else:\n",
    "        raise RuntimeError(\"Not recognized which parameter\")\n",
    "\n",
    "    # We basically select all the columns available\n",
    "    X = data.values[:, 0:2]\n",
    "    scores = data.values[:, 2:].astype(float)\n",
    "    y = np.mean(scores, axis=1)\n",
    "    sd = np.std(scores, axis=1)\n",
    "\n",
    "    return Bunch(X=X.astype(\"object\"), y=y, sd=sd)\n",
    "\n",
    "\n",
    "def fetch_SimLex999():\n",
    "    \"\"\"\n",
    "    Fetch SimLex999 dataset for testing attributional similarity\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : sklearn.datasets.base.Bunch\n",
    "        dictionary-like object. Keys of interest:\n",
    "        'X': matrix of 2 words per column,\n",
    "        'y': vector with scores,\n",
    "        'sd': vector of sd of scores,\n",
    "        'conc': matrix with columns conc(w1), conc(w2) and concQ the from dataset\n",
    "        'POS': vector with POS tag\n",
    "        'assoc': matrix with columns denoting free association: Assoc(USF) and SimAssoc333\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Hill, Felix et al., \"Simlex-999: Evaluating semantic models with (genuine) similarity estimation\", 2014\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "     SimLex-999 is a gold standard resource for the evaluation of models that learn the meaning of words and concepts.\n",
    "     SimLex-999 provides a way of measuring how well models capture similarity, rather than relatedness or\n",
    "     association. The scores in SimLex-999 therefore differ from other well-known evaluation datasets\n",
    "     such as WordSim-353 (Finkelstein et al. 2002). The following two example pairs illustrate the\n",
    "     difference - note that clothes are not similar to closets (different materials, function etc.),\n",
    "     even though they are very much related: coast - shore 9.00 9.10, clothes - closet 1.96 8.00\n",
    "    \"\"\"\n",
    "    _partial_fetch('SimLex999 dataset: attributional similarity')\n",
    "    data = _get_as_pd('https://www.dropbox.com/s/0jpa1x8vpmk3ych/EN-SIM999.txt?dl=1',\n",
    "                      'similarity', sep=\"\\t\")\n",
    "\n",
    "    # We basically select all the columns available\n",
    "    X = data[['word1', 'word2']].values\n",
    "    y = data['SimLex999'].values\n",
    "    sd = data['SD(SimLex)'].values\n",
    "    conc = data[['conc(w1)', 'conc(w2)', 'concQ']].values\n",
    "    POS = data[['POS']].values\n",
    "    assoc = data[['Assoc(USF)', 'SimAssoc333']].values\n",
    "\n",
    "    return Bunch(X=X.astype(\"object\"), y=y, sd=sd, conc=conc, POS=POS, assoc=assoc)\n",
    "\n",
    "\n",
    "def fetch_TR9856():\n",
    "    \"\"\"\n",
    "    Fetch TR9856 dataset for testing multi-word term relatedness\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : sklearn.datasets.base.Bunch\n",
    "        dictionary-like object. Keys of interest:\n",
    "        'X': matrix of 2 words per column,\n",
    "        'y': vector with scores,\n",
    "        'topic': vector of topics providing context for each pair of terms\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Levy, Ran et al., \"TR9856: A multi-word term relatedness benchmark\", 2015.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    \"\"\"\n",
    "    _partial_fetch('TR9856 dataset: multi-word term relatedness')\n",
    "    data = pd.read_csv(os.path.join(_fetch_file(\n",
    "        'https://www.research.ibm.com/haifa/dept/vst/files/IBM_Debater_(R)_TR9856.v2.zip',\n",
    "        'similarity', uncompress=True, verbose=0),\n",
    "        'IBM_Debater_(R)_TR9856.v0.2', 'TermRelatednessResults.csv'), encoding=\"iso-8859-1\")\n",
    "\n",
    "    # We basically select all the columns available\n",
    "    X = data[['term1', 'term2']].values\n",
    "    y = data['score'].values\n",
    "    topic = data['topic'].values\n",
    "\n",
    "    return Bunch(X=X.astype(\"object\"), y=y, topic=topic)\n",
    "  \n",
    "def evaluate_similarity(w, X, y):\n",
    "    \"\"\"\n",
    "    Calculate Spearman correlation between cosine similarity of the model\n",
    "    and human rated similarity of word pairs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : Embedding or dict\n",
    "      Embedding or dict instance.\n",
    "\n",
    "    X: array, shape: (n_samples, 2)\n",
    "      Word pairs\n",
    "\n",
    "    y: vector, shape: (n_samples,)\n",
    "      Human ratings\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cor: float\n",
    "      Spearman correlation\n",
    "    \"\"\"\n",
    "    if isinstance(w, dict):\n",
    "        w = Embedding.from_dict(w)\n",
    "\n",
    "    missing_words = 0\n",
    "    words = w.vocabulary.word_id\n",
    "    for query in X:\n",
    "        for query_word in query:\n",
    "            if query_word not in words:\n",
    "                missing_words += 1\n",
    "    #if missing_words > 0:\n",
    "     #   logger.warning(\"Missing {} words. Will replace them with mean vector\".format(missing_words))\n",
    "\n",
    "\n",
    "    mean_vector = np.mean(w.vectors, axis=0, keepdims=True)\n",
    "    A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
    "    B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n",
    "    scores = np.array([v1.dot(v2.T)/(np.linalg.norm(v1) * np.linalg.norm(v2)) for v1, v2 in zip(A, B)])\n",
    "    return scipy.stats.spearmanr(scores, y).correlation\n",
    "  \n",
    "  \n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MEN dataset: similarity and relatedness\n",
      "Downloading WS353 dataset: attributional and relatedness similarity\n",
      "Downloading SimLex999 dataset: attributional similarity\n",
      "Downloading MTurk dataset: attributional similarity\n",
      "\n",
      " MEN : 3000 items\n",
      "     sun, sunlight : [10.]\n",
      "     automobile, car : [10.]\n",
      "     river, water : [9.8]\n",
      "     stair, staircase : [9.8]\n",
      "\n",
      " WS353R : 252 items\n",
      "     computer, keyboard : 7.62\n",
      "     Jerusalem, Israel : 8.46\n",
      "     planet, galaxy : 8.11\n",
      "     canyon, landscape : 7.53\n",
      "\n",
      " SimLex999 : 999 items\n",
      "     old, new : 1.58\n",
      "     smart, intelligent : 9.2\n",
      "     hard, difficult : 8.77\n",
      "     happy, cheerful : 9.55\n",
      "\n",
      " MTurk : 287 items\n",
      "     episcopal, russia : 5.5\n",
      "     water, shortage : 5.428571428\n",
      "     horse, wedding : 4.533333334\n",
      "     plays, losses : 6.4\n"
     ]
    }
   ],
   "source": [
    "# custom functions\n",
    "\n",
    "similarity_tasks = {\n",
    "    \"MEN\": fetch_MEN(),\n",
    "    \"WS353R\": fetch_WS353(which=\"relatedness\"),\n",
    "    \"SimLex999\": fetch_SimLex999(),\n",
    "    \"MTurk\": fetch_MTurk(),\n",
    "}\n",
    "\n",
    "for name, dataset in similarity_tasks.items():\n",
    "    print('\\n', name, ':',len(dataset.X),'items')\n",
    "    for data, score in zip(dataset.X[:4], dataset.y[:4]):\n",
    "        print(' '*4, ', '.join(data), ':', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultats évaluation intrinsèque\n",
    "\n",
    "Notre objectif est de comparer les similarités entre les paires de mots des datasets calculées à partir des embeddings et celles données par les annotateurs humains. Si un embedding prédit les similarités de la même manière que les humains, on estime qu'il est bon. On peut donc calculer la corrélation entre la proximité donné par l'embedding et celle donnée par les humains pour chaque paire de mots du dataset.\n",
    "\n",
    "Pour cet excercice, nous allons utiliser  le classe [Embeddings](https://polyglot.readthedocs.io/en/latest/polyglot.mapping.html#module-polyglot.mapping.embeddings) de polyglot. Pour charger un embeddind avec cette classe : \n",
    "\n",
    "`glove_embeddings =  Embedding.from_glove('data/glove.6B.50d.txt')`\n",
    "\n",
    "Pour pouvoir charger les embeddings de Collobert de la même manière, il faut mettre les mots et les vecteurs dans un seul fichier, par exemple avec la commande linux `paste`:\n",
    "\n",
    "`paste -d ' ' collobert_words.lst collobert_embeddings.txt > collobert.txt`\n",
    "\n",
    "\n",
    "\n",
    "#### Question\n",
    "\n",
    "> * pour chaque embedding Collober et Glove, et chaque dataset (MEN, WS353R, SimLex999 et MTurk), calculer la similarité entre les proximités données par l'embedding et celles données par les humains. On utilisera la fonction `similarity.evaluate_similarity(word_embeddings, dataset.X, dataset.y)` qui renvoit le [coefficient de correlation de Spearman](https://fr.wikipedia.org/wiki/Corr%C3%A9lation_de_Spearman).\n",
    "> * stocker les scores  pour chaque embedding et chaque dataset dans une liste `similarity_results = []` sous forme d'un dictonnaire : `similarity_results.append({'Embeddings': embeddings_name, 'Dataset': name, 'Score': score})`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding functions\n",
    "from polyglot.mapping import Embedding\n",
    "\n",
    "similarity_results = []\n",
    "\n",
    "# Load both embeddings with Embedding.from_glove from Polyglot\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Loop on embeddings\n",
    "for embeddings_name, embeddings in [('collobert', collobert_embeddings), ('glove', glove_embeddings)]:\n",
    "    # loop on tasks\n",
    "    for name, dataset in similarity_tasks.items():\n",
    "        # compute similarity\n",
    "        # YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des résultats de similarité\n",
    "\n",
    "Le code suivant permet de visualiser les coefficients de corrélation pour chaque dataset sur les différents jeux de test.\n",
    "\n",
    "#### Question\n",
    "> * Quel est selon ces métriques le meilleur embedding ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(similarity_results, orient='columns')\n",
    "df\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('#f9f9f9')\n",
    "\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize':(8, 6)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "colors = [\"#e74c3c\", \"#75d9fc\", \"#b4e0ef\", \"#34495e\", \"#e74c3c\", \"#2ecc71\"]\n",
    "ax = sns.barplot(x=\"Dataset\", y=\"Score\", hue=\"Embeddings\", data=df, errwidth=0, palette=sns.color_palette(colors))\n",
    "\n",
    "\n",
    "ax.legend(loc=9, bbox_to_anchor=(0.5, -0.1), ncol=3, fancybox=True, shadow=False)\n",
    "ax.set(xlabel=\"\", ylabel=\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation d'analogies\n",
    "\n",
    "Notre objectif est maintenant d'explorer les relations sémantiques induites par l'arithmétique sur les embeddings. Nous allons donc explorer les analogies induites par les embeddings sous forme de raisonnement du type : \"l'homme est au roi ce que la femme est à ?\", la réponse étant \"la reine\". On peut calculer la réponse avec les représentations fournies par l'embedding par :  \n",
    "\n",
    "`v = vecteur(roi)-vecteur(homme)+vecteur(femme)`. \n",
    "\n",
    "La réponse étant alors le mot dont la représentation est la plus proche du vecteur `v`. Pour trouver le mot dont le vecteur est le plus proche de `v`, il faut définir une distance dans l'espace des embeddings. Nous utiliserons la [similarité cosinus](https://fr.wikipedia.org/wiki/Similarit%C3%A9_cosinus)\n",
    "\n",
    "#### Question\n",
    ">* Implémenter la similarity cosinus à l'aide des fonctions [np.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html#numpy.dot) et [np.linalg.norm](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html#numpy.linalg.norm)\n",
    ">* Appliquer le calcul d'analogies sur les triplets proposés ou ceux de votre choix. Observez-vous [ce phénomène](https://arxiv.org/pdf/1607.06520.pdf) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cosine_similarity(a,b):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def sorted_by_similarity(word_embeddings, base_vector):\n",
    "    \"\"\"Returns words sorted by cosine distance to a given vector, most similar first\"\"\"\n",
    "    words_with_distance = [(my_cosine_similarity(base_vector, word_embeddings[w]), w) \n",
    "                           for w in word_embeddings.vocabulary]\n",
    "\n",
    "    return sorted(words_with_distance, key=lambda t: t[0], reverse=True)\n",
    "\n",
    "def is_redundant(word):\n",
    "    return (\n",
    "        word_1.lower() in word.lower() or\n",
    "        word_2.lower() in word.lower() or\n",
    "        word_3.lower() in word.lower())\n",
    "\n",
    "\n",
    "pairs = [(['man', 'woman'], 'king'), \n",
    "         (['man', 'programmer'], 'woman'), \n",
    "         (['father', 'doctor'], 'mother'),\n",
    "         (['father', 'facebook'], 'mother')\n",
    "        ]\n",
    "\n",
    "words_and_responses = []\n",
    "\n",
    "# Note : you may need to update the following line with your Polyglot Embeddings\n",
    "for embeddings_name, embeddings in [('collobert', collobert_embeddings), ('glove', glove_embeddings)]:\n",
    "    for pair in pairs:\n",
    "        word_1, word_2, word_3 = pair[0][0], pair[0][1], pair[1]\n",
    "        \n",
    "        closest = sorted_by_similarity(embeddings, \n",
    "                                       embeddings[word_2] - embeddings[word_1] + \n",
    "                                       embeddings[word_3])[:10]\n",
    "\n",
    "        closest = [(dist, w) for (dist, w) in closest if not is_redundant(w)] #\n",
    "        \n",
    "        print(\"{} + {} - {} = ? {}\".format(word_2, word_3, word_1, closest[0][1]))\n",
    "        words_and_responses += [word_1, word_2, word_3,closest[0][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des analogies\n",
    "\n",
    "Les relations d'analogies peuvent se visualiser dans l'espace des embeddings après réduction de dimension, par exemple avec tSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note : you may need to update the following line with your Polyglot Embeddings\n",
    "for embeddings_name, embeddings in [('collobert', collobert_embeddings), ('glove', glove_embeddings)]:\n",
    "    \n",
    "    word_vectors = np.array([embeddings[word] for word in words_and_responses[:4]])\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=0, n_iter=1000, perplexity=3.0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    T = tsne.fit_transform(word_vectors)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.patch.set_facecolor('#f9f9f9')\n",
    "\n",
    "    sns.set(rc={'figure.figsize':(6, 6)})\n",
    "    sns.set(font_scale=1.3)\n",
    "\n",
    "    sns.scatterplot(x=T[:, 0], y=T[:, 1])\n",
    "    \n",
    "    for label, x, y in zip(words_and_responses, T[:, 0], T[:, 1]):\n",
    "        plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des embeddings de BERT\n",
    "\n",
    "BERT a été un des premiers modèles de langue Transformer, entraînés sur de gros corpus, disponible librement. De nombreux modèles sont disponibles sur HuggingFace.\n",
    "\n",
    "Comme BERT est un modèle contextuel, il est nécessaire de lui faire prédire des phrases entières pour étudier les embeddings de mots qu'il produit. Dans cette section, nous allons comparer les embeddings obtenus pour des mots polysémiques en fonction de la phrase dans laquelle ils sont utilisés.\n",
    "\n",
    "En anglais, *plant* possède deux sens : celui d'usine et celui d'un végétal. Avec un embedding non contextuel, de type Glove ou Colobert, ces deux sens du mot plus sont associés à un identique embedding. Avec BERT, nous allons voir que le même mot peut avoir plusieurs embeddings en fonction du contexte.\n",
    "\n",
    "First, load the BERT model and tokenizer from HuggingFace : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Load pre-trained model \n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # to access the hidden states\n",
    "                                  )\n",
    "# set the model to \"evaluation\" mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Les modèles de langues sont entrainés avec un découpe spécifique des phrases en token. Ces tokens peuvent être des mots ou des parties de mots. Il est nécessaire d'utiliser le tokenizer correspondant à chaque model.\n",
    "\n",
    "tokenizer.vocab.keys() donne la liste de tous les tokens connus du modèle de langue. \n",
    "\n",
    "#### Question\n",
    ">* combien de token différents sont connu du tokenizer de BERT ?\n",
    ">* affichez une centaine de token aléatoirement. Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# number of token in tokenizer\n",
    "# YOU CODE HERE\n",
    "# sample of 100 tokens\n",
    "# YOU CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tokenizer découpe les phrases et transforme les éléments (mots ou sous-mots) en indice. \n",
    "\n",
    "BERT peut traiter plusieurs phrases mais il faut lui indiquer le découpage en phrases (segment) avec un indice : 0 pour la première phrases, 1 pour la deuxième. \n",
    "\n",
    "Deux tokens spécifiques doivent être aussi ajoutés : \n",
    "* [CLS], un token spécifique utilisé pour la classification de phrase\n",
    "* [SEP], le token de fin de phrase.\n",
    "\n",
    "#### Question\n",
    ">* Appliquer la fonction bert_tokenize sur les 3 phases et conservez les 3 vecteurs (index, token, segment)\n",
    ">* Affichez ces informations pour chacune des phrases et vérifier que le mot *plant* a bien le même indice de token dans les deux phrases où il apparait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt1 = \"The plant has reached its maximal level of production.\"\n",
    "snt2 = \"The cars are assembled inside the factory.\"\n",
    "snt3 = \"A plant needs sunlight and water to grow well.\"\n",
    "\n",
    "\n",
    "def bert_tokenize(snt):\n",
    "    \"\"\" Apply the BERT tokenizer to a list of words representing a sentence\n",
    "        and return 3 lists: \n",
    "        - list of token indx\n",
    "        - list of token for debugging, not used by the BERT model\n",
    "        - list of sentence index\n",
    "        \"\"\"\n",
    "    # Add the special tokens.\n",
    "    tagged_snt = \"[CLS] \" + snt + \" [SEP]\" \n",
    "    # Tokenize\n",
    "    tokenized_snt = tokenizer.tokenize(tagged_snt)\n",
    "    # convert tokens to indices\n",
    "    indexed_snt = tokenizer.convert_tokens_to_ids(tokenized_snt)\n",
    "    # mark the words in sentence.\n",
    "    segments_ids = [1] * len(tokenized_snt)\n",
    "\n",
    "    return (indexed_snt, tokenized_snt, segments_ids)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inférence\n",
    "\n",
    "Pour calculer les embeddings, il est nécessaire de faire une prédiction avec le modèle BERT sur une phrase complète. La fonction *predict_hidden* convertit les listes d'indices de token et de segment en tenseur pytorch et applique le modèle. \n",
    "\n",
    "Le modème utilisé est un modèle à 12 couches. Nous allons utiliser la dernière couche caché du modèle comme embedding pour représenter les mots. D'autres solutions serait possible, comme une concaténation ou une moyene de plusieurs couches.\n",
    "\n",
    "\n",
    "#### Question\n",
    ">* Appliquer le modèle à chacune des 3 phrases et stocker les embeddings obtenus (tenseurs)\n",
    ">* Afficher la dimension des tenseurs obtenus. Quelle est la dimension du vecteur d'embedding pour chaque mot ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_hidden(indexed_snt, segments_ids):\n",
    "    \"\"\"Apply the BERT model to the input token indices and segment indices\n",
    "        and return the last hidden layer\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_snt])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "        one_hidden_layer = hidden_states[12][0]\n",
    "        \n",
    "    return one_hidden_layer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La couche cachée renvoyée par la fonction *predict_hidden* est un tenseur contenant pour chaque token de la phrase d'entrée un vecteur contextuel le représentant. On peut utiliser ce vecteur pour représenter le sens de ce mot en fonction de son contexte. Nous allons comparer la représentation du mot polysémique *plant* en fonction de son contexte.\n",
    "\n",
    "#### Question\n",
    ">* En utilisant la [distance cosinus](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html), calculer les distances suivantes:\n",
    ">   * distance entre *plant* dans la phrase 1 (plant-factory) et *plant* dans la phrase 3 (plant-vegetal)\n",
    ">   * distance entre *plant* dans la phrase 1 (plant-factory) et *factory* dans la phrase 2 \n",
    ">   * distance entre *plant* dans la phrase 1 (plant-factory) et *production* dans la phrase 2 \n",
    ">   * distance entre *plant* dans la phrase 3 (plant-vegetal) et *production* dans la phrase 2 \n",
    "> * Comment interprêter ces distances ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
